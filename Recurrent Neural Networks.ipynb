{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks with LSTM\n",
    "This ipython notebook implements Forward Propagation of Recurrent Neural network with Long-Short Term Memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing important libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Functions\n",
    "Implementing basic functions like Sigmoid and Softmax which are used by different Functions to forward propagate in Recurrent Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    s = e_z / np.sum(e_z)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurent Cell:\n",
    "<img src=\"image1.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recurrent_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"This function computes the activation a_next and output yt_pred of current time step 't' using \n",
    "    activation from previous time step i.e. a_prev: a numpy array of shape n_a * m where m is number of training examples.\n",
    "    i.e. xt: a numpy array of shape nx * m where m is the number of examples and nx is the length of one-hot vector. \n",
    "    and the current input. \n",
    "    The parameters are retrieved from a dictionary named parameters having following parameters for a layer:\n",
    "    Wax: Weight Matrix Multiplied by input xt.\n",
    "    Waa: Weight Matrix Multiplied by previous hidden state activation a_prev.\n",
    "    Wya: Weight Matrix Multiplied by current hidden state activation a_next to calculate output.\n",
    "    ba: Bias to compute activation a_next\n",
    "    by: Bias to compute output yt_pred\"\"\"\n",
    "    \n",
    "    # Retrieving the parameters from parameters dictionary\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Waa,a_prev) + np.dot(Wax,xt) + ba) # Current State activation\n",
    "    yt_pred = softmax(np.dot(Wya,a_next) + by) # Current State Output\n",
    "    \n",
    "    # Storing Values in a cache, can be used while back-propagating.\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation in Recurrent Neural Network\n",
    "<img src=\"image2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image3.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recurrent_forward_propagation(x, a0, parameters):\n",
    "    \"\"\"This function implements forward propagation from time step 1 to t using recurrent_cell_forward function.\n",
    "    x : input\n",
    "    a0: Time step 0 activation -> A vector of Zeros.\n",
    "    parameters: Parameter Dictionary.\"\"\"\n",
    "    \n",
    "    caches = [] # Initializing Caches which will contain list of cache for every time step.\n",
    "    \n",
    "    # Retrieving Shapes from x and parameters.\n",
    "    n_x, m, T_x = x.shape  # n_x is length of a training example, m is number of examples, T_x is number of time steps.\n",
    "    n_y, n_a = parameters[\"Wya\"].shape # n_y is number of outputs, n_a is length of activation a.\n",
    "    \n",
    "    # Initializing Activations across time-steps and training examples.\n",
    "    a = np.zeros((n_a, m, T_x)) # Contains Activations \n",
    "    y_pred = np.zeros((n_y, m, T_x)) # Contains Outputs\n",
    "    \n",
    "    # Initializing a_next which consists current state activation.\n",
    "    a_next = a0 # a0 is zeros of shape n_a , m\n",
    "    \n",
    "    # Looping over the time steps:\n",
    "    for t in range(T_x):\n",
    "        a_next, yt_pred, cache = recurrent_cell_forward(x[:,:,t], a_next, parameters) # Computing values from recurrent_cell_forward function.\n",
    "        \n",
    "        a[:,:,t] = a_next # Saving the value of activation of current time step in a.\n",
    "        \n",
    "        y_pred[:,:,t] = yt_pred # Saving the value of output of current time step in y_pred.\n",
    "        \n",
    "        caches.append(cache) # Appending cache to caches list.\n",
    "        \n",
    "        \n",
    "    # Storing caches list in caches tuple with x, to be used in backward propagation.\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Cell\n",
    "<img src=\"image4.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"This function computes forward propagation for a single long short term memory cell\n",
    "    Function Parameters: xt: Current input, a_prev: Previous time step activation, \n",
    "    c_prev: Memory Cell of previos time step and parameters : Dictionary of Parameters.\"\"\"\n",
    "    \n",
    "    # Retrieving Parameters\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wf = parameters[\"Wf\"] # Weights to compute forget gate\n",
    "    bf = parameters[\"bf\"] # Bias for forget gate\n",
    "    Wi = parameters[\"Wi\"] # Weights to compute update gate\n",
    "    bi = parameters[\"bi\"] # Bias for update gate\n",
    "    Wc = parameters[\"Wc\"] # Weights to compute candidate value of memory cell.\n",
    "    bc = parameters[\"bc\"] # Bias for candidate value of memory cell.\n",
    "    Wo = parameters[\"Wo\"] # Weights to compute output gate.\n",
    "    bo = parameters[\"bo\"] # Bias for output gate.\n",
    "    Wy = parameters[\"Wy\"] # Weights to compute output at current time step.\n",
    "    by = parameters[\"by\"] # Bias to compute output at current time step.\n",
    "    \n",
    "    # Retrieving dimensions from shapes of xt and Wy\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "    \n",
    "    # Concatenate a_prev and xt. To take dot product directly with Wf,Wi,Wc to \n",
    "    # compute forget gate,update gate and candidate value respectively.\n",
    "    concat = np.zeros((n_a + n_x , m ))\n",
    "    concat[: n_a, :] = a_prev\n",
    "    concat[n_a :, :] = xt\n",
    "    \n",
    "    ft = sigmoid(np.dot(Wf , concat)  + bf)  # Forget Gate\n",
    "    it = sigmoid(np.dot(Wi , concat)  + bi)  # Update Gate \n",
    "    cct = np.tanh(np.dot(Wc , concat)  + bc) # Candidate value of memory cell.\n",
    "    c_next = np.multiply(ft, c_prev) + np.multiply(it, cct) # Cnext\n",
    "    ot = sigmoid(np.dot(Wo , concat)  + bo) # Output Gate\n",
    "    a_next = np.multiply(ot, np.tanh(c_next)) # Current State Activation.\n",
    "    \n",
    "    yt_pred = softmax(np.dot(Wy, a_next) + by) # Prediction at Current State.\n",
    "    \n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) # Storing Values in cache to be used in Backpropagation.\n",
    "    \n",
    "    return a_next, c_next, yt_pred, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward Propagation for LSTM.\n",
    "def lstm_forward_propagation(x, a0, parameters):\n",
    "    \"\"\"This function computes forward propagation for LSTM Cells for time steps 1 to T using lstm_cell_forward function\n",
    "        Function Parameters: x: Input of shape (n_x, m, T_x) \n",
    "                             a0: Initial hidden state, of shape (n_a, m)\n",
    "                             parameters: Dictionary containing parameters.\"\"\"\n",
    "    \n",
    "    # Initialize \"caches\", which will track the list of all the caches\n",
    "    caches = []\n",
    "    \n",
    "    # Retrieve dimensions from shapes of x and parameters['Wy'] \n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wy\"].shape\n",
    "    \n",
    "    # initialize \"a\", \"c\" and \"y\" with zeros \n",
    "    a = np.zeros((n_a, m, T_x)) # Activations of All Lstm Cells i.e. T time steps for m examples.\n",
    "    c = np.zeros((n_a, m, T_x)) # Memory Cells of All Lstm Cells i.e. T time steps for m examples.\n",
    "    y = np.zeros((n_y, m, T_x)) # Predictions of All Lstm Cells i.e. T time steps for m examples.\n",
    "    \n",
    "    # Initialize a_next and c_next \n",
    "    a_next = a0 # a_next is current state activation.\n",
    "    c_next = np.zeros(a_next.shape) # c_next is current state memory cell.\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, next memory state, compute the prediction, get the cache\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a \n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y \n",
    "        y[:,:,t] = yt\n",
    "        # Save the value of the next cell state \n",
    "        c[:,:,t]  = c_next\n",
    "        # Append the cache into caches\n",
    "        caches.append(cache)\n",
    "        \n",
    "        # store values needed for backward propagation in cache\n",
    "        caches = (caches, x)\n",
    "        \n",
    "        return a, y, c, caches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
